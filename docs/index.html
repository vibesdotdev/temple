<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>TEMPLE — Crystallographic Projections in Neural Network Weight Space</title>
  <meta name="description" content="TEMPLE explores geometric lattice constraints on LLM weights, producing measurable output diversity with less than 1% benchmark degradation.">
  <link rel="stylesheet" href="style.css">
</head>
<body>

<header class="site-header">
  <nav>
    <a href="#" class="logo">TEMPLE</a>
    <a href="#abstract">Abstract</a>
    <a href="#method">Method</a>
    <a href="#experiments">Experiments</a>
    <a href="#geometry-trajectories">Trajectories</a>
    <a href="#discussion">Discussion</a>
    <a href="#bibtex">Cite</a>
  </nav>
</header>

<main>

  <!-- Hero -->
  <div class="hero">
    <div class="project-name">TEMPLE &mdash; Research Report &mdash; 2026</div>
    <h1>Crystallographic Projections in Neural Network Weight Space</h1>
    <p class="authors">
      Andrew Ewing &middot; Independent Research &middot; 2026
    </p>
    <div class="links">
      <a href="https://github.com/vibesdotdev/temple" class="btn btn-primary">Code</a>
      <a href="#abstract" class="btn">Abstract</a>
      <a href="#experiments" class="btn">Results</a>
    </div>
  </div>

  <!-- Abstract -->
  <section id="abstract">
    <h2>Abstract</h2>
    <div class="abstract-box">
      <p>
        We introduce <strong>TEMPLE</strong> (Training with Enforced Manifold Projections on Lattice Embeddings),
        a method that imposes crystallographic lattice geometry on neural network weight matrices, either
        post-hoc or as a constraint during gradient descent. Three crystal symmetries are explored: cubic
        (regular Cartesian grid), hexagonal (1D dual-grid), and quasicrystalline (cut-and-project along an
        irrational slope). Applied post-hoc to Qwen3-4B-Instruct at displacement scale &sigma;&nbsp;=&nbsp;0.005,
        crystallized models score within <strong>0.86% of baseline</strong> on ARC-Challenge (89.51% base vs.
        88.65% quasi) and within <strong>2.7pp on MMLU</strong> (8-task average: 73.0% base vs. 70.2–72.2%
        crystallized) while producing outputs with roughly <strong>75% word-level divergence</strong>
        (word-type Jaccard&nbsp;&asymp;&nbsp;0.25) from the base model. From-scratch training with lattice-constrained
        gradient descent converges to geometry-specific behavioral attractor basins, suggesting that
        weight-space geometry constitutes a meaningful, parameterizable dimension of model behavior—
        one that is <em>free</em> in terms of capability cost at practical scales.
      </p>
    </div>
  </section>

  <!-- Introduction -->
  <section id="introduction">
    <h2>1. Introduction</h2>
    <p>
      The dominant approaches to steering language model behavior—RLHF, DPO, instruction tuning—require
      expensive supervised data and full training runs. Activation steering and representation engineering
      are cheaper but introduce per-token runtime overhead. Prompting is trivially cheap but coarse.
    </p>
    <p>
      We ask a different question: <em>can the geometry of weight space itself encode behavioral
      diversity?</em> Crystallographic lattices are among the most well-studied geometric structures
      in mathematics and physics. They offer a principled, continuously parameterized family of
      discrete constraints, each with distinct symmetry properties.
    </p>
    <p>
      TEMPLE applies these constraints to neural weights via a nearest-neighbor projection: each weight
      value is snapped to the closest point on a scaled lattice. The key properties of this operation
      are (1) it is <strong>deterministic and reproducible</strong>, (2) it is <strong>parameter-free
      at inference time</strong> (weights are modified once, at save time), and (3) different crystal
      symmetries produce outputs that are <strong>measurably distinct</strong> from one another and
      from the base model, while preserving task performance at moderate displacement scales.
    </p>
    <p>
      This work reports early experimental results across two settings: post-hoc crystallization of a
      pretrained 4B parameter instruct model, and from-scratch training of 123M parameter models under
      lattice gradient constraints. A 1B parameter from-scratch run is ongoing.
    </p>
  </section>

  <!-- Method -->
  <section id="method">
    <h2>2. Method</h2>

    <h3>2.1 Lattice Projection</h3>
    <p>
      Given a weight tensor <strong>W</strong> and scale parameter &sigma;, the lattice projection
      <strong>&Pi;<sub>L</sub></strong> maps each scalar weight to the nearest point on lattice
      <em>L</em> scaled by &sigma;:
    </p>
    <pre><code>&#x3A0;_L(w; &#x3C3;) = &#x3C3; &middot; nearest_L(w / &#x3C3;)</code></pre>
    <p>
      This is applied independently to each weight value. The scale &sigma; controls the granularity
      of quantization—larger &sigma; means coarser snapping and larger weight displacement.
    </p>

    <h3>2.2 Crystal Geometries</h3>

    <h4>Cubic</h4>
    <p>
      The simplest lattice: a regular Cartesian grid. Projection is equivalent to rounding to the
      nearest multiple of &sigma;.
    </p>
    <pre><code>w &rarr; round(w / &#x3C3;) &times; &#x3C3;</code></pre>

    <h4>Hexagonal</h4>
    <p>
      A 1D dual-grid lattice using two Fibonacci chains with offsets <code>[0, 0]</code> and
      <code>[0, &#x3C3;/2]</code>. For each weight, candidate lattice points are drawn from both
      chains and the nearest is selected. This produces an irregular but deterministic spacing
      that breaks the regular symmetry of the cubic grid.
    </p>

    <h4>Quasicrystalline</h4>
    <p>
      A 1D cut-and-project lattice following a Penrose-style construction. Integers are projected
      onto the line <em>y = x/&phi;</em> (where &phi; is the golden ratio) and the nearest projected
      lattice point is selected. The resulting spacing is aperiodic but long-range ordered, with no
      repeating unit cell—the defining property of a quasicrystal.
    </p>
    <p class="note" style="margin-top:0.5em">
      <strong>Known limitation:</strong> the current implementation is not idempotent—applying the
      projection twice changes ~9% of values (max &Delta;&nbsp;&asymp;&nbsp;0.0025 at &sigma;=0.005).
      For post-hoc experiments (single application) this has no effect. For from-scratch training
      (projection every 10 steps) it means the constraint accumulates additional drift rather than
      enforcing a fixed attractor; behavioral results from training runs should be interpreted as
      "quasicrystalline-inspired" rather than strictly lattice-constrained. A v2 implementation
      using an explicit Fibonacci grid will address this.
    </p>

    <h3>2.3 Post-hoc Application</h3>
    <p>
      For a pretrained model, we load all weight tensors, apply the lattice projection, and
      re-save the model. The operation is applied to all linear layer weights; embedding matrices
      and layer norms are excluded. Total modification time for a 4B parameter model is under
      two minutes on an M3 Ultra.
    </p>
    <p>
      Quantized models (MLX 8-bit) require special handling: scale/bias tensors returned by
      <code>mx.quantize</code> must be cast back to their original dtype before saving, and
      SafeTensors metadata must include <code>{"format": "mlx"}</code> or downstream tools
      will reject the file.
    </p>

    <h3>2.4 From-scratch Training with Lattice Constraints</h3>
    <p>
      For from-scratch training, the projection is applied after each gradient step as a
      straight-through estimator: gradients flow as if the weights were unconstrained, but
      the stored values are always snapped to the nearest lattice point. This is equivalent
      to quantization-aware training with a crystal-specific codebook.
    </p>
    <p>
      We train a 123M parameter transformer on the Fineweb-Edu dataset using MLX on Apple
      Silicon. The architecture is a standard decoder with rotary positional embeddings.
    </p>
  </section>

  <!-- Experiments -->
  <section id="experiments">
    <h2>3. Experiments</h2>

    <h3>3.1 Post-hoc Crystallization of Qwen3-4B</h3>
    <p>
      We apply all three crystal geometries to Qwen3-4B-Instruct (MLX 8-bit quantized) at
      &sigma;&nbsp;=&nbsp;0.005 and evaluate on ARC-Challenge using the lm-evaluation-harness
      with zero-shot chat completion.
    </p>

    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Model</th>
            <th class="num">ARC-Challenge<br><small>n=1172</small></th>
            <th class="num">&Delta; ARC</th>
            <th class="num">MMLU avg<br><small>8 tasks, n=50</small></th>
            <th class="num">&Delta; MMLU</th>
          </tr>
        </thead>
        <tbody>
          <tr class="highlight-row">
            <td>Base (Qwen3-4B)</td>
            <td class="num best">89.51%</td>
            <td class="num">&mdash;</td>
            <td class="num best">73.0%</td>
            <td class="num">&mdash;</td>
          </tr>
          <tr>
            <td>Cubic (&sigma;=0.005)</td>
            <td class="num">89.25%</td>
            <td class="num delta-neg">&minus;0.26pp</td>
            <td class="num">70.2%</td>
            <td class="num delta-neg">&minus;2.7pp</td>
          </tr>
          <tr>
            <td>Hexagonal (&sigma;=0.005)</td>
            <td class="num">89.25%</td>
            <td class="num delta-neg">&minus;0.26pp</td>
            <td class="num">72.2%</td>
            <td class="num delta-neg">&minus;0.7pp</td>
          </tr>
          <tr>
            <td>Quasicrystalline (&sigma;=0.005)</td>
            <td class="num">88.65%</td>
            <td class="num delta-neg">&minus;0.86pp</td>
            <td class="num">71.0%</td>
            <td class="num delta-neg">&minus;2.0pp</td>
          </tr>
        </tbody>
      </table>
    </div>

    <p>
      Across both benchmarks, crystallized models retain <strong>96–99% of base capability</strong>
      at &sigma;&nbsp;=&nbsp;0.005 (6% average weight displacement). Hexagonal geometry achieves the
      smallest MMLU degradation (&minus;0.7pp), while all three geometries remain within noise on ARC.
      Notably, all crystallized models <em>outperform</em> base on high school psychology (92%&nbsp;&rarr;&nbsp;94%),
      and hexagonal matches or beats base on four of eight MMLU subtasks.
    </p>

    <h3>3.2 MMLU Subtask Breakdown</h3>
    <p>
      8 representative subtasks (50 questions each) evaluated via direct chat API with
      zero-shot single-letter prompting. Delta shown relative to base.
    </p>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Subtask</th>
            <th class="num">Base</th>
            <th class="num">Cubic</th>
            <th class="num">Hex</th>
            <th class="num">Quasi</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>Abstract Algebra</td>
            <td class="num">48.0%</td>
            <td class="num delta-neg">44.0% <small>(&minus;4)</small></td>
            <td class="num delta-neg">42.0% <small>(&minus;6)</small></td>
            <td class="num delta-neg">44.0% <small>(&minus;4)</small></td>
          </tr>
          <tr>
            <td>High School Physics</td>
            <td class="num">68.0%</td>
            <td class="num">68.0% <small>(&nbsp;0)</small></td>
            <td class="num delta-pos">70.0% <small>(+2)</small></td>
            <td class="num">68.0% <small>(&nbsp;0)</small></td>
          </tr>
          <tr>
            <td>College Biology</td>
            <td class="num">84.0%</td>
            <td class="num delta-neg">82.0% <small>(&minus;2)</small></td>
            <td class="num delta-neg">82.0% <small>(&minus;2)</small></td>
            <td class="num delta-neg">82.0% <small>(&minus;2)</small></td>
          </tr>
          <tr>
            <td>College Computer Science</td>
            <td class="num">68.0%</td>
            <td class="num delta-neg">64.0% <small>(&minus;4)</small></td>
            <td class="num">68.0% <small>(&nbsp;0)</small></td>
            <td class="num delta-neg">62.0% <small>(&minus;6)</small></td>
          </tr>
          <tr>
            <td>Formal Logic</td>
            <td class="num">62.0%</td>
            <td class="num delta-neg">56.0% <small>(&minus;6)</small></td>
            <td class="num delta-pos">64.0% <small>(+2)</small></td>
            <td class="num">62.0% <small>(&nbsp;0)</small></td>
          </tr>
          <tr>
            <td>Clinical Knowledge</td>
            <td class="num">80.0%</td>
            <td class="num delta-neg">74.0% <small>(&minus;6)</small></td>
            <td class="num delta-neg">74.0% <small>(&minus;6)</small></td>
            <td class="num delta-neg">72.0% <small>(&minus;8)</small></td>
          </tr>
          <tr>
            <td>World Religions</td>
            <td class="num">82.0%</td>
            <td class="num delta-neg">80.0% <small>(&minus;2)</small></td>
            <td class="num delta-pos">84.0% <small>(+2)</small></td>
            <td class="num delta-pos">84.0% <small>(+2)</small></td>
          </tr>
          <tr>
            <td>High School Psychology</td>
            <td class="num">92.0%</td>
            <td class="num delta-pos">94.0% <small>(+2)</small></td>
            <td class="num delta-pos">94.0% <small>(+2)</small></td>
            <td class="num delta-pos">94.0% <small>(+2)</small></td>
          </tr>
        </tbody>
        <tfoot>
          <tr>
            <td><strong>Average</strong></td>
            <td class="num"><strong>73.0%</strong></td>
            <td class="num delta-neg"><strong>70.2%</strong> <small>(&minus;2.7)</small></td>
            <td class="num delta-neg"><strong>72.2%</strong> <small>(&minus;0.7)</small></td>
            <td class="num delta-neg"><strong>71.0%</strong> <small>(&minus;2.0)</small></td>
          </tr>
        </tfoot>
      </table>
    </div>

    <h3>3.2 Scale Study</h3>
    <p>
      We sweep the displacement scale &sigma; across seven values for the quasicrystalline geometry,
      measuring weight displacement, output divergence (word-type Jaccard similarity to base, lower = more
      divergent), and coherence (type-token ratio of generated text).
    </p>

    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>&sigma;</th>
            <th class="num">Displacement</th>
            <th class="num">Jaccard Similarity &darr;</th>
            <th class="num">Coherence</th>
            <th>Character</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td><code>0.001</code></td>
            <td class="num">1.2%</td>
            <td class="num">0.445</td>
            <td class="num">0.547</td>
            <td>Minimal divergence</td>
          </tr>
          <tr>
            <td><code>0.002</code></td>
            <td class="num">2.4%</td>
            <td class="num">0.296</td>
            <td class="num">0.529</td>
            <td>Emerging divergence</td>
          </tr>
          <tr>
            <td><code>0.005</code></td>
            <td class="num">6.0%</td>
            <td class="num">0.377</td>
            <td class="num">0.540</td>
            <td>Non-monotonic&dagger;</td>
          </tr>
          <tr class="highlight-row">
            <td><code>0.010</code></td>
            <td class="num">11.9%</td>
            <td class="num">0.252</td>
            <td class="num">0.569</td>
            <td>&#x2713; Sweet spot</td>
          </tr>
          <tr class="cliff-row">
            <td><code>0.020</code></td>
            <td class="num">22.9%</td>
            <td class="num">0.132</td>
            <td class="num">0.374</td>
            <td>&#x26A0; Coherence cliff</td>
          </tr>
          <tr class="cliff-row">
            <td><code>0.050</code></td>
            <td class="num">54.6%</td>
            <td class="num">0.028</td>
            <td class="num">0.187</td>
            <td>Severe degradation</td>
          </tr>
          <tr class="cliff-row">
            <td><code>0.100</code></td>
            <td class="num">86.7%</td>
            <td class="num">0.000</td>
            <td class="num">0.045</td>
            <td>Incoherent output</td>
          </tr>
        </tbody>
      </table>
    </div>

    <p style="font-size: 13px; color: var(--text-muted);">
      &dagger; The non-monotonic Jaccard at &sigma;=0.005 (higher similarity than &sigma;=0.002)
      suggests that mid-range perturbations may preserve semantic content while altering surface
      form, a pattern worth investigating further.
    </p>

    <div class="insight">
      <strong>Key finding</strong>
      The coherence cliff between &sigma;=0.010 (coherence 0.569) and &sigma;=0.020 (coherence 0.374)
      is abrupt—a 34% drop for a 13% increase in displacement. This suggests a critical threshold
      in the model's ability to route through lattice-perturbed weights, and defines a practical
      operating envelope for post-hoc crystallization.
    </div>

    <h3>3.3 From-scratch Training</h3>
    <p>
      We train 123M parameter models from random initialization under both quasicrystalline lattice
      constraints and an unconstrained (amorphous) baseline. Both models are trained on 164M tokens
      from the Fineweb-Edu dataset for 15,000 steps at 14k tokens/sec on an M3 Ultra.
    </p>

    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Model</th>
            <th class="num">Parameters</th>
            <th class="num">Tokens</th>
            <th class="num">Final Loss</th>
            <th>Observed Attractor</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>Amorphous (baseline)</td>
            <td class="num">123M</td>
            <td class="num">164M</td>
            <td class="num">4.003</td>
            <td>"you can't do anything" loops — grammatically coherent repetition</td>
          </tr>
          <tr>
            <td>Quasicrystalline</td>
            <td class="num">123M</td>
            <td class="num">164M</td>
            <td class="num">~5.98</td>
            <td>"I am my my my" — fragmented comma cascades, different structural pattern</td>
          </tr>
        </tbody>
      </table>
    </div>

    <p>
      Both models are undertrained (the Chinchilla-optimal token budget for 123M parameters is
      approximately 2.5B tokens). However, the models converge to <em>different</em> degenerate
      attractors: the amorphous baseline falls into grammatically fluent but semantically empty
      repetition, while the quasicrystalline model produces a structurally distinct failure mode
      with shorter, fragmented patterns. This distinction persists across varied prompts and
      suggests the geometry is influencing the loss landscape in a stable, reproducible way.
    </p>

    <div class="note">
      <strong>In progress:</strong> A 1B parameter quasicrystalline model is currently training
      (step ~10,000 of 50,000 as of this writing). Results will be added when training completes.
    </div>

    <h3>3.4 Output Diversity — Qualitative</h3>
    <p>
      At &sigma;=0.01, the post-hoc crystallized models respond to the same prompt with noticeably
      different outputs. To illustrate, the scale study used a fixed set of creative prompts
      including associative chains, dream-logic vignettes, and geometric poetry. The base model's
      response to "write a poem contrasting liquid and crystal" produced structured quatrains
      with direct metaphor; crystallized variants produced different rhythmic choices, different
      associative leaps, and different conclusions—while remaining coherent and on-topic.
    </p>
    <p>
      Systematic personality/style evaluation (e.g., Big Five trait assessment across diverse
      prompt sets, inter-prompt consistency measurement) is planned as next-step validation.
    </p>
  </section>

  <!-- Geometry & Semantic Trajectories -->
  <section id="geometry-trajectories">
    <h2>4. Perturbation Geometry Selects Semantic Trajectory</h2>

    <p>
      The most striking finding from this work emerged from a control experiment designed
      to falsify the central claim. A natural objection to any weight perturbation result
      is that the observed effects are generic properties of perturbation near a decision
      boundary—that any noise of sufficient magnitude would produce the same behavior,
      and the crystal geometry is decorative.
    </p>
    <p>
      To test this, we ran <strong>displacement-matched isotropic Gaussian noise</strong>
      as a baseline: for each lattice scale &sigma;, we computed the Gaussian &sigma; that
      produces the same average relative weight displacement, then ran identical prompts
      with identical greedy decoding. If geometry is irrelevant, the trajectories should
      match. They do not.
    </p>

    <h3>4.1 The Control Experiment</h3>
    <p>
      Using a word association probe—"give me the first word that comes to mind for X"
      with greedy (temp=0) decoding—we measured how the model's primary association shifts
      as displacement magnitude increases under three perturbation regimes: quasicrystalline
      lattice, cubic lattice, and Gaussian noise.
    </p>

    <h4>Word: LATTICE</h4>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>&sigma;</th>
            <th class="num">Displacement</th>
            <th>Quasi</th>
            <th>Cubic</th>
            <th>Gaussian</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td><code>base</code></td>
            <td class="num">—</td>
            <td colspan="3"><strong>grid</strong> — "a lattice and a grid are both structured arrangements of lines or points"</td>
          </tr>
          <tr>
            <td><code>0.001</code></td>
            <td class="num">1.2%</td>
            <td>grid</td>
            <td><strong>structure</strong></td>
            <td>grid</td>
          </tr>
          <tr>
            <td><code>0.002</code></td>
            <td class="num">2.4%</td>
            <td><strong>structure</strong></td>
            <td>grid</td>
            <td>grid</td>
          </tr>
          <tr class="highlight-row">
            <td><code>0.005</code></td>
            <td class="num">6.0%</td>
            <td>GRID</td>
            <td><strong>CRYSTAL</strong></td>
            <td>GRID</td>
          </tr>
          <tr class="highlight-row">
            <td><code>0.010</code></td>
            <td class="num">11.9%</td>
            <td><strong>crystal</strong></td>
            <td><strong>MESH</strong></td>
            <td>GRID</td>
          </tr>
          <tr class="cliff-row">
            <td><code>0.020</code></td>
            <td class="num">22.9%</td>
            <td colspan="2"><em>coherence collapse</em></td>
            <td><em>coherence collapse</em></td>
          </tr>
        </tbody>
      </table>
    </div>

    <p>
      Gaussian noise never leaves "grid" until coherence collapses entirely. Both lattice
      methods traverse the semantic neighborhood — but via <em>different routes</em>. Cubic
      reaches "crystal" first (at 6% displacement) then overshoots to "mesh." Quasi arrives
      at "crystal" later (11.9%) and settles. They cross each other: at &sigma;=0.005, cubic
      is on crystal while quasi has reverted to grid; at &sigma;=0.010 they have swapped.
    </p>

    <div class="insight">
      <strong>Key observation</strong>
      The divergence at &sigma;=0.002 is sharp. At 2.4% displacement, quasi has moved to
      "structure" (an abstraction of grid) while Gaussian stays on "grid." The full output
      text confirms: quasi's explanation invokes crystallography as a conceptual frame;
      Gaussian's explanation stays at the visual/structural synonym level. Same displacement
      magnitude, different semantic depth. The geometry is not just adding noise—it is
      reranking within an existing neighborhood, promoting higher-abstraction neighbors
      over surface-level ones.
    </div>

    <h3>4.2 Replication on a Geometry-Free Word</h3>
    <p>
      "Lattice" has direct semantic connections to the perturbation method. To rule out
      neighborhood-specific confounding, we ran the same protocol on <strong>"dog"</strong>—
      a high-frequency word with a strong, unambiguous primary association ("bark") and
      no geometric content.
    </p>

    <h4>Word: DOG</h4>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>&sigma;</th>
            <th class="num">Displacement</th>
            <th>Quasi</th>
            <th>Cubic</th>
            <th>Gaussian</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td><code>base</code></td>
            <td class="num">—</td>
            <td colspan="3"><strong>bark</strong> — "a dog is known for barking"</td>
          </tr>
          <tr>
            <td><code>0.001</code></td><td class="num">1.2%</td>
            <td>bark</td><td>bark</td><td>bark</td>
          </tr>
          <tr>
            <td><code>0.002</code></td><td class="num">2.4%</td>
            <td>bark</td><td>bark</td><td>bark</td>
          </tr>
          <tr>
            <td><code>0.005</code></td><td class="num">6.0%</td>
            <td>bark</td><td>bark</td><td>bark</td>
          </tr>
          <tr class="highlight-row">
            <td><code>0.010</code></td>
            <td class="num">11.9%</td>
            <td>bark</td>
            <td><strong>CAT</strong></td>
            <td><strong>fur</strong></td>
          </tr>
          <tr class="cliff-row">
            <td><code>0.020</code></td>
            <td class="num">22.9%</td>
            <td colspan="3"><em>coherence collapse</em></td>
          </tr>
        </tbody>
      </table>
    </div>

    <p>
      "Bark" is a stronger attractor than "grid" — all three methods hold it through 6%
      displacement. At 11.9%, the methods diverge into <em>three interpretably distinct
      types of association</em>:
    </p>

    <ul style="padding-left: 20px; margin-bottom: 20px;">
      <li style="margin-bottom: 10px;">
        <strong>Quasi → bark</strong> (unchanged): the aperiodic projection does not cross
        the boundary at this scale. The primary behavioral association holds.
      </li>
      <li style="margin-bottom: 10px;">
        <strong>Cubic → CAT</strong>: moves to the canonical <em>categorical neighbor</em>.
        Dog and cat occupy the same semantic category (pet animals); cubic's orthogonal
        axes appear to traverse the categorical dimension.
      </li>
      <li style="margin-bottom: 10px;">
        <strong>Gaussian → fur</strong>: moves to a <em>perceptual/physical property</em>.
        Isotropic noise degrades the relational structure and surfaces a feature of the
        referent rather than a conceptual neighbor.
      </li>
    </ul>

    <div class="insight">
      <strong>Interpretation</strong>
      The three perturbation methods are traversing different <em>dimensions</em> of the
      semantic neighborhood simultaneously. Cubic selects categorical adjacency. Gaussian
      selects perceptual salience. Quasi maintains the primary association deeper into
      perturbation space before collapse. This is not consistent with a generic "noise
      near a decision boundary" explanation—that account predicts the same trajectory
      regardless of perturbation structure, differing only in rate. Instead, the geometry
      of the perturbation appears to select which axes of the semantic space get traversed.
    </div>

    <h3>4.3 Coherence is Matched Across Methods</h3>
    <p>
      A critical control: the divergent trajectories are not explained by one method
      degrading output quality faster than another. Coherence scores (type-token ratio
      × length factor) are within measurement error across all three methods at every
      displacement level up to the cliff at &sigma;=0.020. The geometry is changing
      <em>direction</em>, not <em>energy</em>.
    </p>

    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>&sigma;</th>
            <th class="num">Quasi coherence</th>
            <th class="num">Cubic coherence</th>
            <th class="num">Gaussian coherence</th>
          </tr>
        </thead>
        <tbody>
          <tr><td><code>0.001</code></td><td class="num">0.547</td><td class="num">0.544</td><td class="num">0.536</td></tr>
          <tr><td><code>0.002</code></td><td class="num">0.529</td><td class="num">0.537</td><td class="num">0.570</td></tr>
          <tr><td><code>0.005</code></td><td class="num">0.540</td><td class="num">0.516</td><td class="num">0.540</td></tr>
          <tr><td><code>0.010</code></td><td class="num">0.569</td><td class="num">0.552</td><td class="num">0.571</td></tr>
          <tr class="cliff-row"><td><code>0.020</code></td><td class="num">0.374</td><td class="num">0.544</td><td class="num">0.427</td></tr>
        </tbody>
      </table>
    </div>
    <p style="font-size:13px; color: var(--text-muted);">
      Note: cubic maintains coherence at &sigma;=0.020 (0.544) while quasi and Gaussian
      have collapsed (0.374, 0.427), consistent with orthogonal projections preserving
      more independent weight dimensions deeper into perturbation space.
    </p>

    <h3>4.4 Full Word Battery</h3>
    <p>
      A 40-word battery covering diverse semantic categories (animals, objects, natural
      phenomena, abstract concepts, social roles) with no geometric content is currently
      running. Results will replace this placeholder with aggregate trajectory-type
      statistics and, if the pattern holds, a blind categorization study testing whether
      shift type (categorical / perceptual / functional / hierarchical) recovers geometry
      above chance.
    </p>
    <div class="note">
      <strong>In progress:</strong> 40-word association battery across all three
      perturbation methods. Estimated completion: overnight.
      <span class="badge badge-running" style="margin-left: 8px;">Running</span>
    </div>

  </section>

  <!-- Discussion -->
  <section id="discussion">
    <h2>4. Discussion</h2>

    <h3>What this is</h3>
    <p>
      A post-hoc, zero-cost (at inference time) method for producing a family of behaviorally
      distinct model variants from a single pretrained checkpoint. The capability cost is under
      1% on ARC-Challenge at the operating scale, and the output divergence is substantial:
      approximately 75% of generated tokens differ between the base model and quasicrystalline
      variant at &sigma;=0.01.
    </p>

    <h3>What this is not (yet)</h3>
    <p>
      A proven method for <em>controllable</em> personality modification. We can demonstrate that
      crystallized models are different from the base and from each other, but we have not yet
      demonstrated that specific geometries reliably produce specific, predictable behavioral
      shifts. The relationship between crystal symmetry and behavioral character is the open
      scientific question.
    </p>

    <h3>Relation to prior work</h3>
    <p>
      <strong>Activation steering / representation engineering</strong> (Zou et al., 2023; Turner
      et al., 2023) modifies model behavior by adding vectors to residual stream activations at
      inference time. TEMPLE operates in weight space instead, eliminating per-token overhead.
    </p>
    <p>
      <strong>LoRA / DPO / RLHF</strong> require supervised data and training compute. TEMPLE
      requires neither—it is a deterministic weight transformation.
    </p>
    <p>
      <strong>Weight quantization</strong> is structurally similar (snapping weights to discrete
      values) but motivated by memory reduction rather than behavioral diversity. Quantization
      uses uniform grids (cubic); TEMPLE's contribution is exploring non-cubic symmetries and
      framing the operation as behavioral parameterization rather than compression.
    </p>

    <h3>Limitations</h3>
    <ul style="padding-left: 20px; margin-bottom: 14px;">
      <li style="margin-bottom: 8px;">Results are from a single model family (Qwen3-4B) and one benchmark family (ARC-Challenge + in-progress GSM8K/MMLU). Generalization is not established.</li>
      <li style="margin-bottom: 8px;">The from-scratch models are severely undertrained; attractor differences may not generalize to well-trained models.</li>
      <li style="margin-bottom: 8px;">"Behavioral diversity" is currently measured by word-type Jaccard overlap (set-based, not subword-token-level)—a shallow metric. Human evaluation and semantic consistency studies are needed.</li>
      <li style="margin-bottom: 8px;">The non-monotonic scale behavior at &sigma;=0.005 is unexplained.</li>
    </ul>
  </section>

  <!-- Future Work -->
  <section id="future">
    <h2>5. Future Work</h2>

    <div class="table-wrap">
      <table>
        <thead>
          <tr><th>Experiment</th><th>Status</th><th>Hypothesis</th></tr>
        </thead>
        <tbody>
          <tr>
            <td>GSM8K + MMLU (4B post-hoc)</td>
            <td><span class="badge badge-running">Running</span></td>
            <td>Crystallized models will score within 2% of base on math/knowledge benchmarks</td>
          </tr>
          <tr>
            <td>1B quasicrystalline from-scratch</td>
            <td><span class="badge badge-running">Training</span></td>
            <td>Larger model will show more distinct, coherent attractor character</td>
          </tr>
          <tr>
            <td>Personality consistency study</td>
            <td><span class="badge badge-planned">Planned</span></td>
            <td>Each geometry will show consistent directional shifts across diverse prompts</td>
          </tr>
          <tr>
            <td>Crystalline model fusion</td>
            <td><span class="badge badge-planned">Planned</span></td>
            <td>Models sharing a lattice coordinate system may merge more cohesively than amorphous variants</td>
          </tr>
          <tr>
            <td>Multi-family validation</td>
            <td><span class="badge badge-planned">Planned</span></td>
            <td>Capability preservation holds across model families and sizes</td>
          </tr>
          <tr>
            <td>Directed geometry design</td>
            <td><span class="badge badge-planned">Planned</span></td>
            <td>Custom lattice symmetries can be designed to target specific behavioral properties</td>
          </tr>
        </tbody>
      </table>
    </div>
  </section>

  <!-- BibTeX -->
  <section id="bibtex">
    <h2>Citation</h2>
    <p>If you find this work useful, please cite:</p>
    <div class="bibtex">@misc{temple2026,<br>
&nbsp;&nbsp;title={TEMPLE: Crystallographic Projections in Neural Network Weight Space},<br>
&nbsp;&nbsp;author={Andrew Ewing},<br>
&nbsp;&nbsp;year={2026},<br>
&nbsp;&nbsp;url={https://github.com/vibesdotdev/temple},<br>
&nbsp;&nbsp;note={Independent research report}<br>
}</div>
  </section>

</main>

<footer>
  TEMPLE &mdash; 2026 &mdash;
  <a href="https://github.com/vibesdotdev/temple">github.com/vibesdotdev/temple</a>
</footer>

</body>
</html>
